{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "### Naive Bayes Basic Concept.\n",
    "<br>\n",
    "**John**: Hey I think I have won a lottery\n",
    "<br>\n",
    "**Ben**: Why do you think you have won, why you are not sure\n",
    "<br>\n",
    "**John**: Because the machine where I scanned my ticket to check the result is not very accurate, it gives correct result 99% of the time. That's why I am only 99% sure.\n",
    "<br>\n",
    "**Ben**: I think it's not 99% sure, lets calculate. Hoe many lottery tickets were sold and how  many prizes were there\n",
    "<br>\n",
    "**John**: 100,000 tickets were sold and there are 100 prizes\n",
    "<br>\n",
    "**Ben**: So you had (100/100000), .001 chances of winning the lottery before checking the result. That's your prior probability\n",
    "<br>\n",
    "At the end there would be 100 winners and 99900 loosers\n",
    "<br>\n",
    "Assume everybody goes and checks their result on the machine, how many people would have positive result:\n",
    "<br>\n",
    "100 * .99 (not all the Actual winners would get positive result)  + 99900 * .01 (some of the loosers would also get positive result)\n",
    "<br>\n",
    "99 + 999 =1098\n",
    "<br>\n",
    "This means machine would give positive result to 1098 people, but only 99 would be the real winner\n",
    "<br>\n",
    "Therefore if you have got positive result then your chances of being a real winner is 99/1098 = .09 or 9%\n",
    "<br>\n",
    "This is called Naive Bayes theorem.It is defined as\n",
    "<br>\n",
    "** *Below Formula Images are taken from wiki** \n",
    "<img src=\"nb3.png\">\n",
    "<br>\n",
    "**Explaination of formula**\n",
    "<br>\n",
    "result=B, winner = A\n",
    "<br>\n",
    "P(A) = Prior probability (Probability of \"A\" before before \"B\", probability of lottery winner before checking your result = .001\n",
    "<br>\n",
    "P(notA) = 1 - P(A)\n",
    "<br>\n",
    "P(B/A) = Probability of B being positive if A is true, probability of (+) winning result is true when you are winner = .99\n",
    "<br>\n",
    "P(B/notA) = Probability of B being positive if A is not true, probability of (+) winning result is true when you are not a winner = .01\n",
    "<br>\n",
    "P(A/B) = Posterior probability, probability of being a real winner if you get winner result on machine = result = .09\n",
    "<br>\n",
    "Using this formula\n",
    "<br>\n",
    "P(A/B) = (.001 * .99) /((.001 * .99) + (.999*.01)) = .09\n",
    "<br>\n",
    "<br>\n",
    "**Further on Naive Bayes**\n",
    "<br>\n",
    "**John**: So I have only 9% probability of being a real winner.. So what can I do to confirm\n",
    "<br>\n",
    "**Ben**:Do we have any other machine to check lottery result\n",
    "<br>\n",
    "**John**:Yes actually I checked my result again on the differect machine just before coming to you, and it also gave positive result that I am winner\n",
    "<br>\n",
    "**Ben**:WoW. Lets calculate your chances now with the same formula. Now the prior probaility is .09. Which means before checking the result on second machine your chances of being a winner was .09 . Everything else would be same as previous calculation\n",
    "<br>\n",
    "P(Winner/result is winner) = P(A/B) = (.09 * .99) /((.09 * .99) + (.91*.01)) = .907\n",
    "<br>\n",
    "**Ben**: Congratulations, now we are 90.7% sure that you are a real winner. This is what Naive Bayes is all about the more evidence, the more you are sure of something being true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You probability of being a winner after getting winner result 1 time(s) is 0.09016393442622944\n",
      "You probability of being a winner after getting winner result 2 time(s) is 0.9074999999999999\n",
      "You probability of being a winner after getting winner result 3 time(s) is 0.9989714794017902\n",
      "You probability of being a winner after getting winner result 4 time(s) is 0.9999896003148012\n"
     ]
    }
   ],
   "source": [
    "#prior probability\n",
    "PA = .001\n",
    "#accuracy of result\n",
    "PBA = .99\n",
    "#number of times result checked\n",
    "n=4\n",
    "#posterior probability after n evidence\n",
    "for i in range(n):\n",
    "    PAB = (PA*PBA)/((PA*PBA)+((1-PA)*(1-PBA)))\n",
    "    PA = PAB\n",
    "    print (\"You probability of being a winner after getting winner result {} time(s) is {}\".format(i+1,PAB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take an hypothetical example to understand how Naive Bayes works in scikit-learn\n",
    "**Once you understand this, you will be able to write your own Naive Bayes in any programming language**\n",
    "I have done all the calculations in spredsheet as well, the spreadsheet and the ipython notebook can be found on following github directory\n",
    "<br>\n",
    "[Github Link devksingh](http://www.github.com/devksingh/ML_Naive_Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the python library required for naive bayes modelling\n",
    "import numpy as np\n",
    "#we are using BernoulliNB as the data we have is in (0,1) format\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#library used to check accuracy of model prediction\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes for choosing best ingredient for creating new dish** <br>\n",
    "A chef is going to use na√Øve bayes to design a new dish. He has got 4 ingredients, from which he is going to choose and mix few ingredients and then either fry or bake it. Every dish will be tasted by random people and they are going to tell chef if they liked or not liked the food.<br>Here is the test data:\n",
    "<br>\n",
    "<img src=\"nb4.png\">\n",
    "<br>\n",
    "Here is the result of tasting:\n",
    "<br>\n",
    "<img src=\"nb5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test data and result\n",
    "X=np.array([[1,1,0,0,0],[1,1,0,0,1],[1,0,0,1,1],[1,0,0,1,0],[1,0,1,0,1],[1,0,1,0,1],[0,0,0,1,0],[0,0,1,1,0],[0,1,0,1,0],[0,1,0,1,1]])\n",
    "y=np.array([[1],        [1],        [0],        [1],        [0],        [1],        [1],        [0],        [1],        [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For predicting we would use Naive Bayes formula and for given data we would calculate P(A/B) for class \"Like\" and for Class \"Don't Like\" and whichever is greater would be the prediction. That means we will check for given data which is more probable \"Like\" or \"Don't Like\" <br><br>\n",
    "P(A/B) = (P(B/A) * P (A)) / P(B)<br>\n",
    "P(A) is probability of particular class, class-1 is 6 times out of 10 time so P(A) for class \"Like\" is .6<br>\n",
    "P(A/B) ishow many times ingredient was present for that class, for example Ingredient 1 is present 4 times out 6 times of class \"Like<br>\n",
    "We may not use P(B) which means how many time particular ingredient was present during all the 10 times tasting, because it will be same for both the class. For example Ingredient-1 is present 6 times, which is true for both the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anc\\envs\\carnd-term1\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define classifier\n",
    "clf =  BernoulliNB()\n",
    "#fit the classifier with training data\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For using Naive bayes for predicting which ingredients to choose, first we will find P(B/A) for both class (\"Like\" and \"Don't Like\")\n",
    "<br>\n",
    "In the below image you will see that there are two counts \"Actual\" and Modified\" <br>\n",
    "Actual is how many times actually that ingredient was present for that class like for example Ingredient 1 is present 4 times out 6 times of class \"Like<br>\n",
    "Modified is we add \"One\" to the ingredient count and \"Two\" to the total class count. We do this so that in case we have ingredient which is not present at all for particular class then the probability would be zero and at the time of calculating it will make the combined probability as zero because we multiply all the individual feature (ingredient) probability to get the probability of particular class.\n",
    "**scikit-learn also does the same addition, as you can see in below result**\n",
    "<img src=\"nb6.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Feature probability for Like***\n",
      "[ 0.625  0.5    0.25   0.5    0.375]\n",
      "***Feature probability for Don't Like***\n",
      "[ 0.5         0.33333333  0.5         0.66666667  0.66666667]\n"
     ]
    }
   ],
   "source": [
    "feature_prob=np.exp(clf.feature_log_prob_)\n",
    "print(\"***Feature probability for Like***\")\n",
    "print(feature_prob[1])\n",
    "print(\"***Feature probability for Don't Like***\")\n",
    "print(feature_prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "for predicting we would calculation probability or each class and compare, whichever is higher would be the predicted class for that given data<br>\n",
    "for given data this is how we calculate the probaility of each class using formula ((multiply P(B/A) for each feature (ingredient) * P(A))<br> P(A) is .6(6 out of 10) for class \"Like\" and .4(4 out of 10) for class \"Don't Like\" <br> While multiplying if the ingredient is 1 then multiply the P(B/A) of that ingredient, if it's ) then (1-P(B/A) for that ingredient, because if it's not present then P(B/A) would be (1-p)<br>\n",
    "If data is 1,1,1,0,0 the probability of class \"Like\" would be (0.625 x 0.5 x 0.25 x (1-0.5) x (1-0.375)) x 0.6 = 0.0146484375 <br>\n",
    "And for class \"Don't Like\" it would be (0.5 x 0.3333333 x 0.5 x (1-0.666666) x (1-0.666666)) x 0.4 = 0.003703718 <br>\n",
    "Therefore class \"Don't Like\" would have probability of (0.003703718) / ((0.0146484375)+(0.003703718))  = 0.20181317 <br>\n",
    "Therefore class \"Like\" would have probability of (0.0146484375) / ((0.0146484375)+(0.003703718))  = 0.79818683<br>\n",
    "<br>\n",
    "**Prediction is Class \"Like\"**\n",
    "<img src=\"nb7.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20181317  0.79818683]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "test=np.array([[1,1,1,0,0]])\n",
    "print(clf.predict_proba(test))\n",
    "print(clf.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy on training data\n",
    "<img src=\"nb8.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is :   0.8\n"
     ]
    }
   ],
   "source": [
    "pred_train=clf.predict(X)\n",
    "acc=accuracy_score(y, pred_train)\n",
    "print(\"The accuracy on training data is :  \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Naive Bayes\n",
    "We generally use Naive Bayes when we have to check effect on chances after any particular event which is supposed to affect the previous chance. <br>Like we did in case of lottery,earlier the chances of winning was .001 but after checking the first result it became 0.09 and after 2nd result it became 0.907. It's probabilistic approach but why it's called Naive is it assumes that all the features are independent. \n",
    "<br> For example if we are using Naive Bayes to detect spam in a message, it would predict only based on presence of particular word or collection of words, order of the words in message does not have any effect on result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
